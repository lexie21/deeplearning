{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TODOS:\n",
        "- figure out the shape - solid understanding\n",
        "- shape vs size?\n",
        "- check all the device!\n",
        "- train properly\n",
        "- draw image from left to right\n",
        "- optimize code in positional encoding, ..."
      ],
      "metadata": {
        "id": "chea2v-O_pRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# masking\n",
        "inp = torch.randn(4,4)\n",
        "F.softmax(inp.masked_fill(torch.tril(torch.ones(4,4))==0,float(\"-inf\")),dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp-3O-a7fmdu",
        "outputId": "ddf8fda1-1524-495a-f1bb-44493d1fa5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5093, 0.4907, 0.0000, 0.0000],\n",
              "        [0.4135, 0.3646, 0.2219, 0.0000],\n",
              "        [0.2161, 0.3955, 0.2536, 0.1348]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "HFTQ939F1yKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.cpu_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAphbuAzW1vB",
        "outputId": "bcab1346-2853-4974-b13a-3c1634f44e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
        "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=os.cpu_count())\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=os.cpu_count())\n",
        "\n",
        "input_shape = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AsUVTLa7Fgu",
        "outputId": "5a0e0b4a-52db-40e0-c435-e988e95ed460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:12<00:00, 13.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets/\n",
            "Files already downloaded and verified\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NudEVMrnSsft"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,device,max_len=512,d_model=16):\n",
        "        super().__init__()\n",
        "        self.pos_enc = torch.empty(max_len,d_model,requires_grad=False,device=device)\n",
        "        for i in range(self.pos_enc.size(0)):\n",
        "          for j in range(self.pos_enc.size(1)//2):\n",
        "            self.pos_enc[i,2*j] = np.sin(i/10000**(2*j/d_model))\n",
        "            self.pos_enc[i,2*j+1] = np.cos(i/10000**(2*j/d_model))\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        x: transformed input embedding where x.shape = [batch_size, seq_len, data_dim]\n",
        "        \"\"\"\n",
        "        pos_emb = x + self.pos_enc\n",
        "        return pos_emb\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self,q,k,v,mask=None):\n",
        "        \"\"\"\n",
        "        q, k, v = transformed query, key, value\n",
        "        q.shape, k.shape, v.shpae = [batch_size, num_head, seq_len, d=d_model/num_head]\n",
        "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
        "        \"\"\"\n",
        "        scores = q@k.transpose(-1,-2) * (1.0/np.sqrt(k.size(-1))) # B,nh,sl,sl\n",
        "        if mask != None:\n",
        "          # print(\"applying mask...\")\n",
        "          scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "        weights = F.softmax(scores,dim=-1)\n",
        "        attention_value = weights @ v\n",
        "        return attention_value\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model=16,num_head=4):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % num_head == 0, \"check if d_model is divisible by num_head\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_head = num_head\n",
        "        self.d = d_model//num_head # head size\n",
        "        self.scores = ScaledDotProductAttention()\n",
        "        self.q = nn.Linear(d_model, d_model)\n",
        "        self.k = nn.Linear(d_model, d_model)\n",
        "        self.v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self,q,k,v,mask=None):\n",
        "\n",
        "        \"\"\"\n",
        "        q, k, v = pre-transformed query, key, value\n",
        "        q.shape, k.shape, v.shpae = [batch_size, seq_len, d_model]\n",
        "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
        "        \"\"\"\n",
        "        # alternatively, we can put x into nn.Linear(d_model, d_model*3) for compactness\n",
        "        # this approach means that the weights for query, key, value are learnt simultaneously => tradeoff\n",
        "\n",
        "        batch_size, seq_len = q.shape[:-1]\n",
        "        q = self.q(q)\n",
        "        k = self.k(k)\n",
        "        v = self.v(v)\n",
        "        q = q.view(batch_size, seq_len, self.num_head, self.d).transpose(1,2)\n",
        "        k = k.view(batch_size, seq_len, self.num_head, self.d).transpose(1,2)\n",
        "        v = v.view(batch_size, seq_len, self.num_head, self.d).transpose(1,2)\n",
        "        output = self.scores(q,k,v,mask)\n",
        "        output = output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model=16,d_ff=32):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc2(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Masking(nn.Module):\n",
        "\n",
        "    def __init__(self,max_len=512): # device removed\n",
        "        super().__init__()\n",
        "        # self.device = device\n",
        "        self.max_len = max_len\n",
        "        self.register_buffer('mask_matrix',torch.tril(torch.ones(max_len,max_len)).view(1,1,max_len,max_len))\n",
        "        # mask_matrix is registered as a buffer inside our model and is included when we move the model to device => no need to specify device here\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        x.shape = [batch_size, seq_len, data_dim]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1) # for training the 2 are equal, but for generation purpose\n",
        "        mask = self.mask_matrix[:,:,:seq_len,:seq_len]\n",
        "        return mask\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model=16,eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "    def forward(self,x):\n",
        "        xmean = x.mean(-1,keepdim=True)\n",
        "        xvar = x.var(-1,keepdim=True)\n",
        "        xscaled = (x-xmean)/torch.sqrt(xvar + self.eps)\n",
        "        normed = self.gamma*xscaled + self.beta\n",
        "        return normed\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model,num_head=num_head)\n",
        "        self.ln_1 = LayerNormalization(d_model=d_model)\n",
        "        self.ln_2 = LayerNormalization(d_model=d_model)\n",
        "        self.ff = PositionwiseFeedForwardNetwork(d_model=d_model,d_ff=d_ff)\n",
        "\n",
        "\n",
        "    def forward(self,enc):\n",
        "        # recent residual nets normalize first than transform and add\n",
        "        # enc = enc + self.attention(self.ln_1(enc))\n",
        "        # output = enc + self.ff(self.ln_2(enc))\n",
        "\n",
        "        output = self.ln_1(enc + self.attention(enc,enc,enc))\n",
        "        ouput = self.ln_2(enc + self.ff(enc))\n",
        "        return output\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.masked_attention = MultiHeadAttention(d_model=d_model,num_head=num_head)\n",
        "        self.cross_attention = MultiHeadAttention(d_model=d_model,num_head=num_head)\n",
        "        self.ff = PositionwiseFeedForwardNetwork(d_model=d_model,d_ff=d_ff)\n",
        "        self.ln_1 = LayerNormalization(d_model=d_model)\n",
        "        self.ln_2 = LayerNormalization(d_model=d_model)\n",
        "        self.ln_3 = LayerNormalization(d_model=d_model)\n",
        "\n",
        "    def forward(self,enc_output,dec,dec_mask):\n",
        "        output = self.ln_1(dec + self.masked_attention(dec,dec,dec,mask=dec_mask)) # what is the first decoder input?\n",
        "        output = self.ln_2(output + self.cross_attention(dec,enc_output,enc_output))\n",
        "        output = self.ln_3(output + self.ff(output))\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.inp_emb = nn.Linear(input_dim,d_model) # shape = 100,512,16\n",
        "        self.pos_emb = PositionalEncoding(device,max_len=max_len,d_model=d_model)\n",
        "        self.blocks = nn.ModuleList([EncoderLayer(d_model=d_model,num_head=num_head,drop_prob=drop_prob) for _ in range(num_layer)])\n",
        "\n",
        "    def forward(self,x): # x.shape = 100, 512, 3\n",
        "        x = self.inp_emb(x)\n",
        "        x = self.pos_emb(x)\n",
        "        for block in self.blocks:\n",
        "          hidden = block(x)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.inp_emb = nn.Linear(input_dim,d_model)\n",
        "        self.pos_emb = PositionalEncoding(device,max_len=max_len,d_model=d_model)\n",
        "        self.blocks = nn.ModuleList([DecoderLayer(d_model=d_model,num_head=num_head,d_ff=d_ff,drop_prob=drop_prob) for _ in range(num_layer)])\n",
        "        self.lc = nn.Linear(d_model,input_dim)\n",
        "\n",
        "    def forward(self,enc_output,y,y_mask):\n",
        "        inputs = self.inp_emb(y)\n",
        "        inputs = self.pos_emb(inputs)\n",
        "        for block in self.blocks:\n",
        "          inputs = block(enc_output,inputs,y_mask)\n",
        "\n",
        "        output = self.lc(inputs) # shape = B,T,3\n",
        "        return output\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(device, input_dim, num_layer, max_len, d_model, num_head, d_ff, drop_prob)\n",
        "        self.decoder = Decoder(device, input_dim, num_layer, max_len, d_model, num_head, d_ff, drop_prob)\n",
        "        self.mask = Masking(max_len)\n",
        "\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        enc_output = self.encoder(x)\n",
        "        dec_output = self.decoder(enc_output,y,self.mask(y))\n",
        "        return dec_output\n",
        "\n",
        "class ScheduledOptimizer:\n",
        "\n",
        "    def __init__(self,optimizer,d_model=16,warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def update_parameter_and_learning_rate(self):\n",
        "        self.optimizer.step()\n",
        "        self.step_num += 1\n",
        "        self.lr = self.d_model**(-.5) * min(self.step_num**(-.5),self.step_num*self.warmup_steps**(-1.5))\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.lr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "OsMOesKXf2UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94794e72-e01e-49f0-adb8-869b2e6f6b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(device=device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1).to(device)\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(),betas=(.9,.98),eps=1e-9)\n",
        "scheduled_optimizer = ScheduledOptimizer(optimizer,d_model=16)\n"
      ],
      "metadata": {
        "id": "Sc1bZl80fjS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Transformer(device=device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1).to(device)\n",
        "# loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
        "# optimizer = torch.optim.Adam(model.parameters(),betas=(.9,.98),eps=1e-9)\n",
        "# scheduled_optimizer = ScheduledOptimizer(optimizer,d_model=16)\n",
        "\n",
        "\n",
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"num_param:\", total_params)\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    ## train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        if batch_idx % 100 == 0:\n",
        "          print(f\"Batch: {batch_idx}\")\n",
        "\n",
        "        image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device) # shape = 100, 512, 3\n",
        "\n",
        "        y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "        logit = model.forward(x,y_) # shape = 100, 512, 3\n",
        "        cost = loss(logit, y)\n",
        "\n",
        "        total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "        scheduled_optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f w/ Learning Rate: %.5f\"%(i,ave_loss,scheduled_optimizer.lr))\n",
        "\n",
        "    ## test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "\n",
        "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "            # zero first time step to condition first output on prev context\n",
        "            y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "            logit = model.forward(x,y_)\n",
        "            cost = loss(logit, y)\n",
        "\n",
        "            total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "    ave_loss = total_loss/len(test_data)\n",
        "    test_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.3f\"%(i,ave_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GB_ZjvkR4COE",
        "outputId": "2724a4f2-ed96-4bfa-ff82-ac013f11fa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_param: 14435\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 0 Train: 949.065 w/ Learning Rate: 0.00049\n",
            "Epoch 0 Test: 861.078\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 1 Train: 855.985 w/ Learning Rate: 0.00099\n",
            "Epoch 1 Test: 851.825\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 2 Train: 852.876 w/ Learning Rate: 0.00148\n",
            "Epoch 2 Test: 851.287\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 3 Train: 852.602 w/ Learning Rate: 0.00198\n",
            "Epoch 3 Test: 851.590\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 4 Train: 852.475 w/ Learning Rate: 0.00247\n",
            "Epoch 4 Test: 850.921\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 5 Train: 852.267 w/ Learning Rate: 0.00296\n",
            "Epoch 5 Test: 850.513\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 6 Train: 852.013 w/ Learning Rate: 0.00346\n",
            "Epoch 6 Test: 850.578\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 7 Train: 851.649 w/ Learning Rate: 0.00395\n",
            "Epoch 7 Test: 850.368\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 8 Train: 851.246 w/ Learning Rate: 0.00373\n",
            "Epoch 8 Test: 849.653\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 9 Train: 850.964 w/ Learning Rate: 0.00354\n",
            "Epoch 9 Test: 849.954\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 10 Train: 850.490 w/ Learning Rate: 0.00337\n",
            "Epoch 10 Test: 848.689\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 11 Train: 849.344 w/ Learning Rate: 0.00323\n",
            "Epoch 11 Test: 847.081\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 12 Train: 848.313 w/ Learning Rate: 0.00310\n",
            "Epoch 12 Test: 847.902\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 13 Train: 847.847 w/ Learning Rate: 0.00299\n",
            "Epoch 13 Test: 846.426\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 14 Train: 847.611 w/ Learning Rate: 0.00289\n",
            "Epoch 14 Test: 845.847\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 15 Train: 847.296 w/ Learning Rate: 0.00280\n",
            "Epoch 15 Test: 845.670\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 16 Train: 847.157 w/ Learning Rate: 0.00271\n",
            "Epoch 16 Test: 845.623\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 17 Train: 847.001 w/ Learning Rate: 0.00264\n",
            "Epoch 17 Test: 845.374\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 18 Train: 846.906 w/ Learning Rate: 0.00256\n",
            "Epoch 18 Test: 845.276\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 19 Train: 846.819 w/ Learning Rate: 0.00250\n",
            "Epoch 19 Test: 845.422\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 20 Train: 846.705 w/ Learning Rate: 0.00244\n",
            "Epoch 20 Test: 845.346\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 21 Train: 846.597 w/ Learning Rate: 0.00238\n",
            "Epoch 21 Test: 845.403\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 22 Train: 846.539 w/ Learning Rate: 0.00233\n",
            "Epoch 22 Test: 845.130\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 23 Train: 846.441 w/ Learning Rate: 0.00228\n",
            "Epoch 23 Test: 844.902\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 24 Train: 846.391 w/ Learning Rate: 0.00224\n",
            "Epoch 24 Test: 845.083\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 25 Train: 846.343 w/ Learning Rate: 0.00219\n",
            "Epoch 25 Test: 845.102\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 26 Train: 846.268 w/ Learning Rate: 0.00215\n",
            "Epoch 26 Test: 844.876\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 27 Train: 846.202 w/ Learning Rate: 0.00211\n",
            "Epoch 27 Test: 844.883\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 28 Train: 846.171 w/ Learning Rate: 0.00208\n",
            "Epoch 28 Test: 844.839\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 29 Train: 846.130 w/ Learning Rate: 0.00204\n",
            "Epoch 29 Test: 844.606\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 30 Train: 846.057 w/ Learning Rate: 0.00201\n",
            "Epoch 30 Test: 844.702\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 31 Train: 846.015 w/ Learning Rate: 0.00198\n",
            "Epoch 31 Test: 844.886\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 32 Train: 845.987 w/ Learning Rate: 0.00195\n",
            "Epoch 32 Test: 844.553\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 33 Train: 845.923 w/ Learning Rate: 0.00192\n",
            "Epoch 33 Test: 844.859\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 34 Train: 845.911 w/ Learning Rate: 0.00189\n",
            "Epoch 34 Test: 844.374\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 35 Train: 845.873 w/ Learning Rate: 0.00186\n",
            "Epoch 35 Test: 844.316\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 36 Train: 845.812 w/ Learning Rate: 0.00184\n",
            "Epoch 36 Test: 844.390\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 37 Train: 845.806 w/ Learning Rate: 0.00181\n",
            "Epoch 37 Test: 844.301\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 38 Train: 845.767 w/ Learning Rate: 0.00179\n",
            "Epoch 38 Test: 844.674\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 39 Train: 845.725 w/ Learning Rate: 0.00177\n",
            "Epoch 39 Test: 844.416\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 40 Train: 845.706 w/ Learning Rate: 0.00175\n",
            "Epoch 40 Test: 844.615\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 41 Train: 845.683 w/ Learning Rate: 0.00173\n",
            "Epoch 41 Test: 844.353\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 42 Train: 845.640 w/ Learning Rate: 0.00170\n",
            "Epoch 42 Test: 844.352\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 43 Train: 845.590 w/ Learning Rate: 0.00169\n",
            "Epoch 43 Test: 844.284\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 44 Train: 845.568 w/ Learning Rate: 0.00167\n",
            "Epoch 44 Test: 844.428\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 45 Train: 845.561 w/ Learning Rate: 0.00165\n",
            "Epoch 45 Test: 844.151\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 46 Train: 845.499 w/ Learning Rate: 0.00163\n",
            "Epoch 46 Test: 844.215\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 47 Train: 845.488 w/ Learning Rate: 0.00161\n",
            "Epoch 47 Test: 844.093\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 48 Train: 845.457 w/ Learning Rate: 0.00160\n",
            "Epoch 48 Test: 844.677\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 49 Train: 845.411 w/ Learning Rate: 0.00158\n",
            "Epoch 49 Test: 844.315\n",
            "Batch: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-5ed6168e664d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mscheduled_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mscheduled_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameter_and_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zRFz0x3NrhH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 2\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"num_param:\", total_params)\n",
        "fig, axes = plt.subplots(1,num_epoch,figsize=(20,6))\n",
        "for i in range(num_epoch):\n",
        "\n",
        "#     ## train\n",
        "#     model.train()\n",
        "\n",
        "#     count = 0\n",
        "  total_loss = 0\n",
        "  for batch_idx, (image, label) in enumerate(train_loader):\n",
        "      # print(batch_idx)\n",
        "      if batch_idx == 0:\n",
        "        plotter = image\n",
        "        plotter_x = plotter.reshape(-1,3,1024).transpose(1,2)[:,:512,:].to(device)\n",
        "      if batch_idx != 0:\n",
        "        break\n",
        "      print(image.mean())\n",
        "      image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "      x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "      y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "      y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "      logit = model.forward(x,y_)\n",
        "      if batch_idx == 0:\n",
        "        plotter_y = logit\n",
        "      # print(f\"logit stats: {logit.min()}, {logit.max()}\")\n",
        "      cost = loss(logit, y)\n",
        "      # print(cost.item()* y.shape[0] * y.shape[1] * y.shape[2])\n",
        "\n",
        "      total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "      scheduled_optimizer.zero_grad()\n",
        "      cost.backward()\n",
        "      scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "      # j += 1\n",
        "\n",
        "  # ave_loss = total_loss/len(train_data)\n",
        "  # train_loss_list.append(ave_loss)\n",
        "\n",
        "  if i % 1 == 0:\n",
        "    print(i)\n",
        "    print(total_loss/(train_loader.batch_size*(batch_idx+1)),\"\\n\")\n",
        "  #     print(\"\\nEpoch %d Train: %.3f w/ Learning Rate: %.5f\"%(i,ave_loss,scheduled_optimizer.lr))\n",
        "    # plotter_x = plotter.reshape(-1,3,1024).transpose(1,2)[:,:512,:].to(device)\n",
        "    reconstructed = torch.concat([plotter_x,plotter_y],dim=1)\n",
        "    axes[i].imshow(reconstructed.reshape(-1,32,32,3)[0,:,:,:].detach().cpu())\n",
        "    # print(reconstructed.reshape(-1,32,32,3)[0,:,:,:])\n",
        "    axes[i].set_title(f\"epoch {i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQG9gA29XI-a",
        "outputId": "940fce17-c8db-4582-c5ee-08e283a13403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_param: 14435\n",
            "tensor(0.5033)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot images\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(plotter[0,:,:,:].permute(1,2,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "wDurTH9JcCvG",
        "outputId": "388b981c-d908-486c-85dc-2e61f231b72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79005e794730>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Transformer(device=device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1).to(device)\n",
        "# loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
        "# optimizer = torch.optim.Adam(model.parameters(),betas=(.9,.98),eps=1e-9)\n",
        "# scheduled_optimizer = ScheduledOptimizer(optimizer,d_model=16)\n",
        "\n",
        "\n",
        "num_epoch = 5\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"num_param:\", total_params)\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    ## train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        if batch_idx % 100 == 0:\n",
        "          print(f\"Batch: {batch_idx}\")\n",
        "\n",
        "        image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device) # shape = 100, 512, 3\n",
        "\n",
        "        y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "        logit = model.forward(x,y_) # shape = 100, 512, 3\n",
        "        cost = loss(logit, y)\n",
        "\n",
        "        total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "        scheduled_optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f w/ Learning Rate: %.5f\"%(i,ave_loss,scheduled_optimizer.lr))\n",
        "\n",
        "    ## test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "\n",
        "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "            # zero first time step to condition first output on prev context\n",
        "            y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "            logit = model.forward(x,y_)\n",
        "            cost = loss(logit, y)\n",
        "\n",
        "            total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "    ave_loss = total_loss/len(test_data)\n",
        "    test_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.3f\"%(i,ave_loss))\n"
      ],
      "metadata": {
        "id": "sL-FAjyIfxCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "6c1486cd-b908-4a35-9474-045644f656f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_param: 14435\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 0 Train: 852.490 w/ Learning Rate: 0.00148\n",
            "Epoch 0 Test: 850.954\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 1 Train: 852.588 w/ Learning Rate: 0.00198\n",
            "Epoch 1 Test: 851.252\n",
            "Batch: 0\n",
            "Batch: 100\n",
            "Batch: 200\n",
            "Batch: 300\n",
            "Batch: 400\n",
            "\n",
            "Epoch 2 Train: 852.295 w/ Learning Rate: 0.00247\n",
            "Epoch 2 Test: 851.735\n",
            "Batch: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c98d2af99e2c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = B, 512, 3 only care about the last time step in NLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-868f9b1ff996>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# fill out here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-868f9b1ff996>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_output, y, y_mask)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0minpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = B,T,3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-868f9b1ff996>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_output, dec, dec_mask)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# but doesnt this skip connection leak future info?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-868f9b1ff996>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# fill out here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}